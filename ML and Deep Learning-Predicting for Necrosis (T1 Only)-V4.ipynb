{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a1cb6d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sklearn\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aafbff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import (KFold, train_test_split, cross_validate, RepeatedStratifiedKFold,\n",
    "                                     cross_val_score, GroupKFold, StratifiedGroupKFold, StratifiedKFold)\n",
    "from sklearn.metrics import (roc_auc_score, make_scorer, f1_score, confusion_matrix, roc_curve, accuracy_score, \n",
    "                             auc, precision_score)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaeea1a",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9118024",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f10fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize the file paths to hold all the images separated by sequence and SRS Date\n",
    "root = '/shares/mimrtl/Users/Chengnan/SRS_Necrosis'\n",
    "# initialize the file path to hold the extracted images\n",
    "radiomics_path = os.path.join(root, 'Intact_Cohort/Radiomics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd092152",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1_batchfile = os.path.join(radiomics_path, 'T1_SkullStrip_BFC_IntStnd_nonorm_fbw_radiomic_features.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30831d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read T1Bravo data and add T1_ prefix to all the feature columns\n",
    "extractedFeatures = pd.read_csv(T1_batchfile)\n",
    "extractedFeatures = extractedFeatures.add_prefix('T1_')\n",
    "extractedFeatures.rename(columns = {'T1_Mask': 'Mask'}, inplace = True)\n",
    "\n",
    "# Get MRN_Date_lesion to specify a merge key for each lesion\n",
    "extractedFeatures['MRN_Date_Lesion'] = extractedFeatures.apply(lambda row: (row['Mask'].split('/')[-3]\n",
    "                                                               + '_' + row['Mask'].split('/')[-2]\n",
    "                                                               + '_' + row['Mask'].split('/')[-1][:-12]).strip()\n",
    "                                                               , axis=1).astype(str)\n",
    "extractedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bba0d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get patient immunotherapy information\n",
    "patientDetails = pd.read_excel(os.path.join(root, 'necrosis_project_sequences_intact.xlsx'))\n",
    "\n",
    "# Get MRN_Date_lesion to specify a merge key for each lesion\n",
    "patientDetails['MRN_Date_Lesion'] = patientDetails.apply(lambda row: (str(row['MRN']) if len(str(row['MRN'])) == 7 else '0' + str(row['MRN']))\n",
    "                                                         + '_' + str(row['MRI_Date'])[:-9]\n",
    "                                                         + '_' + row['RT_Location'].lower().strip()\n",
    "                                                         , axis=1).astype(str)\n",
    "\n",
    "# Replace the necrosis with label 0 and recurrence with label 1\n",
    "patientDetails = patientDetails.replace('necrosis', 0)\n",
    "patientDetails = patientDetails.replace('recurrence', 1)\n",
    "patientDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d8574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MRN_Date_Lesion to merge radiomic_features with the clinical data\n",
    "allFeatures = pd.merge(extractedFeatures, patientDetails, on='MRN_Date_Lesion', how = 'left')\n",
    "\n",
    "# Make sure the data was merged correctly\n",
    "with pd.option_context('display.max_rows', None):  # more options can be specified also\n",
    "    display(allFeatures[['MRN_Date_Lesion', 'Necrosis_or_Recurrence']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e08e09",
   "metadata": {},
   "source": [
    "### Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f7efc3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Keep only columns for radiomic features, labels, and subject MRN\n",
    "keep = ['Necrosis_or_Recurrence', 'MRN']\n",
    "remove_cols = [feature for feature in allFeatures.columns \n",
    "               if not (feature.startswith(\"T1_original\") or feature in keep)]\n",
    "filteredFeatures = allFeatures.drop(remove_cols, axis = 1)\n",
    "\n",
    "# Remove rows with NA values, which signify missing label or features for that lesion\n",
    "filteredFeatures = filteredFeatures.dropna()\n",
    "filteredFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12289720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data and labels\n",
    "X = filteredFeatures.drop(['MRN', 'Necrosis_or_Recurrence'], axis=1)\n",
    "y = filteredFeatures['Necrosis_or_Recurrence'].astype(int)\n",
    "\n",
    "# Get the MRN for each lesion, which is used later on to do patient-stratification when splitting into train and test\n",
    "mrns = list(filteredFeatures.loc[:, 'MRN'])\n",
    "\n",
    "# Strap into one variable\n",
    "data = (X, y, mrns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f1073",
   "metadata": {},
   "source": [
    "## Wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset by label to do wilcoxon-signed rank test\n",
    "recurrence = X.loc[y==1]\n",
    "necrosis = X.loc[y==0]\n",
    "features = necrosis.columns\n",
    "\n",
    "# Run wilcoxon signed rank test on each of the features to see if statistically significant\n",
    "p_values = []\n",
    "for feature in features:\n",
    "    res = stats.mannwhitneyu(necrosis[feature], recurrence[feature])\n",
    "    p_values.append(res.pvalue)\n",
    "\n",
    "# Correct p values for multiple testing\n",
    "p_values_corrected = fdrcorrection(p_values, alpha=0.05, method='indep', is_sorted=False)[1]\n",
    "\n",
    "# Count # of significant feats\n",
    "count = 0\n",
    "sig_features = []\n",
    "for i in range(len(p_values)):\n",
    "    print(features[i], p_values_corrected[i])\n",
    "    if p_values_corrected[i] <= 0.05:\n",
    "        sig_features.append((features[i], p_values_corrected[i]))\n",
    "        count += 1\n",
    "sig_features = sorted(sig_features, key=lambda x: x[1])\n",
    "print(\"Total of {} significant feature(s) after fdr correction:\\n {}\".format(count, sig_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrence['T1_original_glcm_InverseVariance']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925e636d",
   "metadata": {},
   "source": [
    "## Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a feature\n",
    "feature = 'T1_original_glcm_Id'\n",
    "feature_name = 'glcm_Id'\n",
    "# Creating box plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([recurrence[feature], necrosis[feature]], labels=['Recurrence', 'Necrosis'])\n",
    "plt.title('Box Plot of {} Across Both Cohorts'.format(feature_name))\n",
    "plt.ylabel('{} Values'.format(feature_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6bf956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a feature\n",
    "feature = 'T1_original_glcm_InverseVariance'\n",
    "feature_name = 'glcm_InverseVariance'\n",
    "# Creating box plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([recurrence[feature], necrosis[feature]], labels=['Recurrence', 'Necrosis'])\n",
    "plt.title('Box Plot of {} Across Both Cohorts'.format(feature_name))\n",
    "plt.ylabel('{} Values'.format(feature_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d55613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a feature\n",
    "feature = 'T1_original_glcm_DifferenceAverage'\n",
    "feature_name = 'glcm_DifferenceAverage'\n",
    "# Creating box plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot([recurrence[feature], necrosis[feature]], labels=['Recurrence', 'Necrosis'])\n",
    "plt.title('Box Plot of {} Across Both Cohorts'.format(feature_name))\n",
    "plt.ylabel('{} Values'.format(feature_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf280d3",
   "metadata": {},
   "source": [
    "## Running Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced3922",
   "metadata": {},
   "source": [
    "### Defining CV Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from mrmr import mrmr_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CV based off StratifiedGroupKFold in sklearn, with additional functionality for repeats. \n",
    "Stratification splits the dataset such that each fold has a similar class balance as the original.\n",
    "GroupKFold generartes folds by splitting across groups, such that each group doesn't appear both in the train and\n",
    "test set of that fold. Additionally, each group appears exactly once in the test set across all folds.\n",
    "\"\"\"\n",
    "class RepeatedStratifiedGroupKFold():\n",
    "    def __init__(self, n_splits=5, n_repeats=5, random_state=1):\n",
    "        self.random_state = random_state\n",
    "        self.n_repeats = n_repeats\n",
    "        self.n_splits = n_splits\n",
    "\n",
    "    def split(self, X, y, groups):\n",
    "        tries = 0\n",
    "        X = pd.DataFrame(X)\n",
    "        y = pd.DataFrame(y)\n",
    "        \n",
    "        # For each repeat, generate a new KFold split through a new seed. \n",
    "        # Skip the split if any of folds have only one class in either the train or test set, as this causes\n",
    "        # causes errors in AUC calculation.\n",
    "        # If there are any invalid folds, generate another new KFold split.\n",
    "        for idx in range(self.n_repeats):\n",
    "            invalid = True\n",
    "            while (invalid):\n",
    "                cv = StratifiedGroupKFold(n_splits=self.n_splits, shuffle = True, random_state=self.random_state + tries)\n",
    "                invalid = False\n",
    "                tries = tries + 1\n",
    "                # For each fold, test if train set or test set only has one class\n",
    "                for train_index, test_index in cv.split(X, y, groups):\n",
    "                    trainPosRatio = y.iloc[train_index].mean().item()\n",
    "                    testPosRatio = y.iloc[test_index].mean().item()\n",
    "                    if trainPosRatio == 0 or trainPosRatio == 1 or testPosRatio == 0 or testPosRatio == 1:\n",
    "                        invalid = True\n",
    "                        break\n",
    "            \n",
    "            # Yield StratifiedGroupKFold split for this repeat\n",
    "            for train_index, test_index in cv.split(X, y, groups):\n",
    "                yield train_index, test_index\n",
    "\n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        # of splits = # of splits generated by StratiedGroupKFold * # of repeats\n",
    "        cv = StratifiedGroupKFold(n_splits=self.n_splits, shuffle = True, random_state=self.random_state)\n",
    "        return cv.get_n_splits(X, y, groups) * self.n_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "N_SPLITS = 3\n",
    "N_REPEATS = 20\n",
    "rd=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the CV strategy generates splits with similar class balance to the original dataset, and that \n",
    "# the groups don't appear both in the train and test set of that fold\n",
    "cv = RepeatedStratifiedGroupKFold(n_splits=N_SPLITS, n_repeats=N_REPEATS)\n",
    "originalPosRatio = y.mean().item()\n",
    "\n",
    "trainPosRatio = []\n",
    "testPosRatio = []\n",
    "for fold, (train_idxs, test_idxs) in enumerate(cv.split(X, y, mrns)):\n",
    "    # split data\n",
    "    mrns_train, mrns_test = np.array(mrns)[train_idxs], np.array(mrns)[test_idxs]\n",
    "    if (set(mrns_train).intersection(mrns_test)):\n",
    "        raise Exception(\"ERROR: Group appears in both train and test set of a fold. \")\n",
    "    y_train, y_test = y.iloc[train_idxs], y.iloc[test_idxs]\n",
    "    \n",
    "    # get the class ratio for the train and test\n",
    "    trainPosRatio.append(y.iloc[train_idxs].mean().item())\n",
    "    testPosRatio.append(y.iloc[test_idxs].mean().item())\n",
    "\n",
    "print(\"Original ratio for positive label: {}\".format(originalPosRatio))\n",
    "print(\"Ratio across train folds for pos label: {}+-{}\".format(np.mean(trainPosRatio), np.std(trainPosRatio)))\n",
    "print(\"Ratio across test folds for pos label: {}+-{}\".format(np.mean(testPosRatio), np.std(testPosRatio)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f87b62",
   "metadata": {},
   "source": [
    "## Univariate Logit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_univariate(X, y, mrns, clf):\n",
    "    cv = RepeatedStratifiedGroupKFold(n_splits=N_SPLITS, n_repeats=N_REPEATS, random_state=rd)\n",
    "    \n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    specificity_scores = []\n",
    "    sensitivity_scores = []\n",
    "    f1_scores = []\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.DataFrame(y)\n",
    "    for fold, (train_idxs, test_idxs) in enumerate(cv.split(X, y, mrns)):\n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train_idxs], X.iloc[test_idxs]\n",
    "        y_train, y_test = y.iloc[train_idxs], y.iloc[test_idxs]\n",
    "                    \n",
    "        # fit model\n",
    "        clf.fit(X_train, y_train.values.ravel())\n",
    "        \n",
    "        # calculate scores\n",
    "        train_auc_score = roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1])\n",
    "        train_scores.append(train_auc_score)\n",
    "        \n",
    "        test_auc_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "        test_scores.append(test_auc_score)\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, clf.predict(X_test)).ravel()\n",
    "        specificity = tn / (tn+fp)\n",
    "        sensitivity = tp / (tp+fn)\n",
    "        specificity_scores.append(specificity)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "        \n",
    "        f1score = f1_score(y_test, clf.predict(X_test))\n",
    "        f1_scores.append(f1score)\n",
    "    return (train_scores, test_scores, specificity_scores, sensitivity_scores, f1_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dff8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis\n",
    "lr = LogisticRegression(class_weight='balanced', solver = 'liblinear')\n",
    "AUCforUniLogit = {}\n",
    "for (featureName, featureData) in X.items():\n",
    "    train_scores, test_scores, specificity_scores, sensitivity_scores, f1_scores = cross_val_univariate(featureData, y, mrns, lr)\n",
    "    AUCforUniLogit[featureName] = [np.mean(test_scores), np.mean(specificity_scores), np.mean(sensitivity_scores), np.mean(f1_scores)]\n",
    "\n",
    "# Sort top features by test AUC\n",
    "AUCforUniLogitTop = dict(sorted(AUCforUniLogit.items(), key=lambda item: item[1][0], reverse=True))\n",
    "AUCforUniLogitRank = list(AUCforUniLogitTop.keys())\n",
    "AUCforUniLogitTop10 = dict(sorted(AUCforUniLogit.items(), key=lambda item: item[1][0], reverse=True)[:10])\n",
    "\n",
    "# Display top features AUC, specifity, and sensitivity\n",
    "print(\"{:<50} {:<15} {:<15} {:<15} {:<15}\".format('feature','auc value','specificity', 'sensitivity', 'f1score'))\n",
    "for key, value in AUCforUniLogitTop10.items():\n",
    "    print(\"{:<50} {:<15.4f} {:<15.4f} {:<15.4f} {:<15.4f}\".format(key, value[0], value[1], value[2], value[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2c2d1",
   "metadata": {},
   "source": [
    "## Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(X, y, fs):\n",
    "    \"\"\"\n",
    "    Returns the set of features selected based on the given data and method.\n",
    "    \n",
    "    X: training features\n",
    "    y: training labels\n",
    "    fs: Feature selection method.\n",
    "    \"\"\"\n",
    "    \n",
    "    if fs == None:\n",
    "        return X.columns\n",
    "    elif fs[:4] == 'MRMR':\n",
    "        k = int(fs[4:])\n",
    "        features = mrmr_classif(X, y, k, show_progress=False)\n",
    "        return features\n",
    "    elif isinstance(fs, list):\n",
    "        return fs\n",
    "    else:\n",
    "        raise Exception(\"Feature selection method provided ({}) isn't valid\".format(fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc_curve(tprs, test_auc_scores):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    # Plotting 0.5 AUC line\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "    \n",
    "    # Plotting average roc auc curve across all folds\n",
    "    fpr_points = np.linspace(0, 1, 100)\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    ax.plot(\n",
    "        fpr_points,\n",
    "        mean_tpr,\n",
    "        color=\"b\",\n",
    "        label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (np.mean(test_auc_scores), np.std(test_auc_scores)),\n",
    "        lw=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    # Plotting +1 std and -1 std roc auc curves\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(\n",
    "        fpr_points,\n",
    "        tprs_lower,\n",
    "        tprs_upper,\n",
    "        color=\"grey\",\n",
    "        alpha=0.2,\n",
    "        label=r\"$\\pm$ 1 std. dev.\",\n",
    "    )\n",
    "\n",
    "    ax.set(\n",
    "        xlim=[-0.05, 1.05],\n",
    "        ylim=[-0.05, 1.05],\n",
    "        xlabel=\"False Positive Rate\",\n",
    "        ylabel=\"True Positive Rate\",\n",
    "        title=f\"Mean ROC curve with variability\\n(Positive label 'recurrence')\",\n",
    "    )\n",
    "    ax.axis(\"square\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22380228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeExperiment(data, clfs, fs=None, normalize=True, resample=None, verbose=False, plot_roc=False):\n",
    "    \"\"\"\n",
    "    data: Contains X, y, and mrns\n",
    "    clfs: sklearn classifers to train model\n",
    "    fs: Feature selection method to use\n",
    "    normalize: Whether to normalize the features\n",
    "    resample: Resampling strategy defined\n",
    "    verbose: if true, will display results across each fold\n",
    "    plot_roc: if true, the roc curve for each classifier will be plotted\n",
    "    \"\"\"\n",
    "    # initialize variables\n",
    "    (X, y, mrns) = data\n",
    "    cv = RepeatedStratifiedGroupKFold(n_splits=N_SPLITS, n_repeats=N_REPEATS, random_state=rd)\n",
    "    \n",
    "    # For each classifier, define a dictionary to store results across all cross-validation folds\n",
    "    results = []\n",
    "    for i in range(len(clfs)):\n",
    "        results.append(defaultdict(list))\n",
    "    \n",
    "    tprs = [[] for clf in clfs] # for roc curve\n",
    "        \n",
    "    # Keep track of features selected in each train-test split\n",
    "    all_features = []\n",
    "    \n",
    "    for fold, (train_idxs, test_idxs) in enumerate(cv.split(X, y, mrns)):\n",
    "        \n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train_idxs], X.iloc[test_idxs]\n",
    "        y_train, y_test = y.iloc[train_idxs], y.iloc[test_idxs]\n",
    "        \n",
    "        # Perform feature selection within the training set, if necessary\n",
    "        sel_feats = select_features(X_train, y_train, fs)\n",
    "        # Keep track of the features selected across each fold\n",
    "        all_features.extend(sel_feats)\n",
    "        \n",
    "        # Transform X_train and X_test based on selected features\n",
    "        X_train = X_train[sel_feats]\n",
    "        X_test = X_test[sel_feats]\n",
    "        \n",
    "        # Normalize features, necessary for linear models\n",
    "        if normalize:\n",
    "            orig_columns = X_train.columns\n",
    "            sc = StandardScaler()\n",
    "            \n",
    "            # normalize X_train\n",
    "            X_train = pd.DataFrame(sc.fit_transform(X_train))\n",
    "            X_train.columns = orig_columns\n",
    "            \n",
    "            # normalize X_test\n",
    "            X_test = pd.DataFrame(sc.transform(X_test))\n",
    "            X_test.columns = orig_columns\n",
    "        \n",
    "        # resample if specified\n",
    "        if resample:\n",
    "            X_train, y_train = resample.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Apply classifiers to selected features\n",
    "        for (i, clf) in enumerate(clfs):\n",
    "            # fit model\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            # calculate scores\n",
    "            train_auc_score = roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1])\n",
    "            test_auc_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "            tn, fp, fn, tp = confusion_matrix(y_test, clf.predict(X_test)).ravel()\n",
    "            specificity = tn / (tn+fp)\n",
    "            sensitivity = tp / (tp+fn)\n",
    "            f1score = f1_score(y_test, clf.predict(X_test))\n",
    "            accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "        \n",
    "            # add to results dict\n",
    "            results[i]['train_auc_scores'].append(train_auc_score)\n",
    "            results[i]['test_auc_scores'].append(test_auc_score)\n",
    "            results[i]['test_specificity_scores'].append(specificity)\n",
    "            results[i]['test_sensitivity_scores'].append(sensitivity)\n",
    "            results[i]['f1_scores'].append(f1score)\n",
    "            results[i]['accuracy'].append(accuracy)\n",
    "        \n",
    "            # AUC graph\n",
    "            fpr, tpr, _ = roc_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "            interp_tpr = np.interp(np.linspace(0, 1, 100), fpr, tpr)\n",
    "            interp_tpr[0] = 0.0\n",
    "            tprs[i].append(interp_tpr)    \n",
    "\n",
    "            if verbose:\n",
    "                print(\"Results for classifier {} on Repeat {} and Fold {} \".format(clf, fold // N_SPLITS + 1, fold % N_SPLITS + 1))                \n",
    "                print(\"Class balance for training (post resampling if any): \", Counter(y_train))\n",
    "                print(\"Selected features: \", sel_feats)\n",
    "                print(\"TRAIN SCORE: \", train_auc_score)\n",
    "                print(\"TEST AUC SCORE: \", test_auc_score)\n",
    "                print(\"TEST SPECIFITY: \", specificity)\n",
    "                print(\"TEST SENSITIVITY: \", sensitivity)\n",
    "                print()\n",
    "    \n",
    "    # averages across all fold\n",
    "    print(\"RESULTS FOR ALL CLASSIFIERS USING FEATURE SELECTION METHOD: {}\".format(fs))\n",
    "    print(\"{:<30} {:<15} {:<15} {:<15} {:<15}\".format('clf','auc value','specificity', 'sensitivity', 'f1score'))\n",
    "    for (i, clf) in enumerate(clfs):\n",
    "        (auc_mean, auc_std) = np.mean(results[i]['test_auc_scores']), np.std(results[i]['test_auc_scores'])\n",
    "        (specificity_mean, specificity_std) = np.mean(results[i]['test_specificity_scores']), np.std(results[i]['test_specificity_scores'])\n",
    "        (sensitivity_mean, sensitivity_std) = np.mean(results[i]['test_sensitivity_scores']), np.std(results[i]['test_sensitivity_scores'])\n",
    "        (f1score_mean, f1score_std) = np.mean(results[i]['f1_scores']), np.std(results[i]['f1_scores'])\n",
    "        (accuracy_mean, accuracy_std) = np.mean(results[i]['accuracy']), np.std(results[i]['accuracy'])\n",
    "        \n",
    "        print(\"{:<30} {:.4f}±{:<7.4f} {:.4f}±{:<7.4f} {:<.4f}±{:<7.4f} {:<.4f}±{:<7.4f} {:<.4f}±{:<7.4f}\".format(\n",
    "            clf.__class__.__name__, auc_mean, auc_std, specificity_mean, specificity_std, sensitivity_mean, sensitivity_std,\n",
    "            f1score_mean, f1score_std, accuracy_mean, accuracy_std))\n",
    "        if plot_roc:\n",
    "            plot_auc_curve(tprs[i], results[i]['test_auc_scores'])\n",
    "    \n",
    "    # Get the top k overall features\n",
    "    if fs:\n",
    "        k = int(fs[4:])\n",
    "        print(\"Top {} features across all folds: {}\".format(5, Counter(all_features).most_common(5)))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0017ffe6",
   "metadata": {},
   "source": [
    "## Executing Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de9b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_methods = [None, 'MRMR50','MRMR25','MRMR10', 'MRMR5', 'MRMR3']\n",
    "clfs = [RandomForestClassifier(n_estimators = 300, random_state=rd, class_weight='balanced', n_jobs=-1),\n",
    "       LogisticRegression(penalty='l2', solver='liblinear', class_weight='balanced'),\n",
    "       SVC(kernel='linear', probability=True, class_weight='balanced'),\n",
    "       MLPClassifier(hidden_layer_sizes=(16, 16), random_state=rd, max_iter=1000),\n",
    "       DummyClassifier(strategy=\"stratified\", random_state=rd)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac393ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fs in fs_methods:\n",
    "    executeExperiment(data, clfs, fs=fs, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25c5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
